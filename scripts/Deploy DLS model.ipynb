{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominican-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Install\n",
    "\n",
    "#!cd \"Q:\\CTCR_DeepHTS\\DLS_models\\deepOrganoid_IXM_BestFocus\"\n",
    "#!conda env create --name DLS_deployment_cpu --file dls_windows_gpu_env.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "irish-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import scipy.io.wavfile\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from multiprocessing import cpu_count\n",
    "from matplotlib.pyplot import imread\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tired-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active input file:  P:/MX ImageStore/Lei-organoid/20210917-HCT116-growth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5 import QtCore, QtWidgets\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QFileDialog\n",
    "\n",
    "app = QtCore.QCoreApplication.instance()\n",
    "if app is None:\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "Input_path = QFileDialog.getExistingDirectory(caption = 'Input Directory')\n",
    "\n",
    "print(\"Active input file: \",Input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beautiful-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active directory:  Q:/CTCR_DeepHTS/DLS_models/deepOrganoid\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5 import QtCore, QtWidgets\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QFileDialog\n",
    "\n",
    "app = QtCore.QCoreApplication.instance()\n",
    "if app is None:\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "model_path = QFileDialog.getExistingDirectory(caption = 'Select inference model')\n",
    "model_name = os.path.basename(model_path)\n",
    "os.chdir(model_path)\n",
    "print(\"Active directory: \",model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sudden-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"MKL_NUM_THREADS\"] = str(cpu_count())\n",
    "os.environ[\"MKL_NUM_THREADS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "important-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is a slightly modified version of DLS deployment code\n",
    "# Deep Learning Studio - GUI platform for designing Deep Learning AI without programming\n",
    "# Copyright (C) 2018 Deep Cognition Inc.\n",
    "# All rights reserved.\n",
    "\n",
    "def doResize(options):\n",
    "    resize = None\n",
    "    if options and 'Resize' in options and options['Resize'] == True:\n",
    "        resize = (int(options['Width']), int(options['Height']))\n",
    "    return resize\n",
    "\n",
    "\n",
    "def col_pre_process(data, options):\n",
    "    if len(options.keys()) == 0:\n",
    "        return data\n",
    "    else:\n",
    "        if \"pretrained\" in options and options[\"pretrained\"] != 'None':\n",
    "            from tensorflow.keras.applications import inception_v3, vgg16, vgg19, resnet50\n",
    "            if options[\"pretrained\"] == 'InceptionV3':\n",
    "                data = inception_v3.preprocess_input(data)\n",
    "            elif options[\"pretrained\"] == 'ResNet50':\n",
    "                data = resnet50.preprocess_input(data)\n",
    "            elif options[\"pretrained\"] == 'VGG16':\n",
    "                data = vgg16.preprocess_input(data)\n",
    "            elif options[\"pretrained\"] == 'VGG19':\n",
    "                data = vgg19.preprocess_input(data)\n",
    "\n",
    "        if \"Scaling\" in options and float(options[\"Scaling\"]) != 0 and float(options[\"Scaling\"]) != 1:\n",
    "            data = data / float(options[\"Scaling\"])\n",
    "\n",
    "        if 'Normalization' in options and options['Normalization'] == True:\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            data = data - mean\n",
    "            data = data / std\n",
    "            return data\n",
    "        return data\n",
    "\n",
    "\n",
    "def process_test_input(base_dir, test_raw, data_mapping):\n",
    "    test_data = []\n",
    "    le = None\n",
    "\n",
    "    from tensorflow.keras import backend as K\n",
    "    if K.backend() == 'theano' or K.backend() == 'mxnet':\n",
    "        K.set_image_data_format('channels_first')\n",
    "    else:\n",
    "        K.set_image_data_format('channels_last')\n",
    "\n",
    "    # determine the shape of the data to feed into the network\n",
    "    for i in range(len(data_mapping['inputs'])):\n",
    "        inp_port = data_mapping['inputs']['InputPort' + str(i)]\n",
    "        if inp_port['details'][0]['type'] == 'Image':\n",
    "            col_name = inp_port['details'][0]['name']\n",
    "            if 'options' in inp_port['details'][0]:\n",
    "                options = inp_port['details'][0]['options']\n",
    "            else:\n",
    "                options = {}\n",
    "\n",
    "            resize = doResize(options)\n",
    "            img = imread(test_raw[col_name][0])\n",
    "            input_shape = img.shape\n",
    "\n",
    "            num_channels = 1\n",
    "            if resize:\n",
    "                width, height = resize\n",
    "                if len(input_shape) == 3:\n",
    "                    num_channels = 3\n",
    "            else:\n",
    "                if len(input_shape) == 2:\n",
    "                    width, height = input_shape\n",
    "                else:\n",
    "                    width, height, num_channels = input_shape\n",
    "\n",
    "            test_data.append(np.ndarray((len(test_raw),) +\n",
    "                                        (num_channels, width, height), dtype=np.float32))\n",
    "\n",
    "            for j, filename in enumerate(test_raw[col_name]):\n",
    "                img = imread(filename)\n",
    "                if resize:\n",
    "                    #new_size = tuple((np.array(resize) * 0.99999).astype(int))\n",
    "                    img = np.array(Image.fromarray(img.astype(np.uint8)).resize(resize))\n",
    "                    #img = scipy.misc.imresize(img, resize)\n",
    "                if num_channels != 1:\n",
    "                    img = np.transpose(img, (2, 0, 1))\n",
    "                test_data[i][j] = img\n",
    "\n",
    "            if K.image_data_format() == 'channels_last':\n",
    "                test_data[i] = np.transpose(test_data[i], (0, 2, 3, 1))\n",
    "\n",
    "            test_data[i] = col_pre_process(test_data[i], options)\n",
    "\n",
    "        elif inp_port['details'][0]['type'] == 'Audio':\n",
    "            if 'options' in inp_port['details'][0]:\n",
    "                options = inp_port['details'][0]['options']\n",
    "            else:\n",
    "                options = {}\n",
    "\n",
    "            (rate, data) = scipy.io.wavfile.read(\n",
    "                test_raw[col_name][0])\n",
    "            test_data.append(np.ndarray((len(test_raw),) +\n",
    "                                        data.shape, dtype=data.dtype))\n",
    "\n",
    "            for j, filename in enumerate(test_raw[col_name]):\n",
    "                (rate, data) = scipy.io.wavfile.read(filename)\n",
    "                test_data[i][j] = data\n",
    "            test_data[i] = col_pre_process(test_data[i], options)\n",
    "\n",
    "        elif inp_port['details'][0]['type'] == 'Numpy':\n",
    "            if 'options' in inp_port['details'][0]:\n",
    "                options = inp_port['details'][0]['options']\n",
    "            else:\n",
    "                options = {}\n",
    "\n",
    "            col_name = inp_port['details'][0]['name']\n",
    "            npzFile = np.load(test_raw[col_name][0])\n",
    "            x = npzFile[npzFile.files[0]]\n",
    "            input_shape = x.shape\n",
    "\n",
    "            test_data.append(np.ndarray(\n",
    "                (len(test_raw),) + x.shape, dtype=np.float32))\n",
    "            for j, filename in enumerate(test_raw[col_name]):\n",
    "                npzFile = np.load(filename)\n",
    "                x = npzFile[npzFile.files[0]]\n",
    "                test_data[i][j] = x\n",
    "            test_data[i] = col_pre_process(test_data[i], options)\n",
    "\n",
    "        else:\n",
    "            col_idx = 0\n",
    "            test_data.append(np.ndarray(\n",
    "                (len(test_raw), inp_port['size']), dtype=np.float32))\n",
    "            for col in range(len(inp_port['details'])):\n",
    "                if 'options' in inp_port['details'][col]:\n",
    "                    options = inp_port['details'][col]['options']\n",
    "                else:\n",
    "                    options = {}\n",
    "\n",
    "                col_name = inp_port['details'][col]['name']\n",
    "\n",
    "                if inp_port['details'][col]['type'] == 'Categorical':\n",
    "                    data_col = test_raw[col_name]\n",
    "                    num_categories = len(\n",
    "                        inp_port['details'][col]['categories'])\n",
    "\n",
    "                    le_temp = LabelEncoder()\n",
    "                    le_temp.fit(inp_port['details'][col]['categories'])\n",
    "                    data_col = le_temp.transform(data_col)\n",
    "\n",
    "                    one_hot_array = np.zeros(\n",
    "                        (len(data_col), num_categories), dtype=np.float32)\n",
    "                    one_hot_array[np.arange(len(data_col)), data_col] = 1\n",
    "\n",
    "                    test_data[i][:, col_idx:col_idx +\n",
    "                                 num_categories] = col_pre_process(one_hot_array, options)\n",
    "                    col_idx += num_categories\n",
    "\n",
    "                elif inp_port['details'][col]['type'] == 'Array':\n",
    "                    array = np.array(test_raw[col_name].str.split(\n",
    "                        ';').tolist(), dtype=np.float32)\n",
    "                    test_data[i][:, col_idx:col_idx + array.shape[1]\n",
    "                                 ] = col_pre_process(array, options)\n",
    "                    col_idx += array.shape[1]\n",
    "\n",
    "                else:\n",
    "                    data = test_raw[col_name].values.reshape((len(test_raw), 1))\n",
    "                    test_data[i][:, col_idx:col_idx +\n",
    "                                 1] = col_pre_process(data, options)\n",
    "                    col_idx += 1\n",
    "\n",
    "    # assuming single output, generate labelEncoder\n",
    "    out_port = data_mapping['outputs']['OutputPort0']\n",
    "    if out_port['details'][0]['type'] == 'Categorical':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(out_port['details'][0]['categories'])\n",
    "\n",
    "    return test_data, le\n",
    "\n",
    "\n",
    "def customPredict(test_data, config, modelFile):\n",
    "    res = None\n",
    "    loss_func = config['params']['loss_func']\n",
    "    if 'is_custom_loss' in config['params']:\n",
    "        isCustomLoss = config['params']['is_custom_loss']\n",
    "    else:\n",
    "        isCustomLoss = False\n",
    "\n",
    "    if isCustomLoss:\n",
    "        customLoss = SourceFileLoader(\n",
    "            \"customLoss\", 'customLoss.py').load_module()\n",
    "        loss_function = eval('customLoss.' + loss_func)\n",
    "        mod = load_model(modelFile, custom_objects={loss_func: loss_function})\n",
    "    else:\n",
    "        mod = load_model(modelFile)\n",
    "    \n",
    "    #disabling temporarily\n",
    "    if os.environ.get(\"GPU_ENABLED\", \"0\") == \"1\":\n",
    "        mod.compile (loss='categorical_crossentropy', optimizer='adam', context=[\"GPU(0)\"])\n",
    "    with tf.device(\"GPU:0\"):\n",
    "        ress = mod.predict(test_data,batch_size=4)\n",
    "    return ress\n",
    "\n",
    "\n",
    "def test_model(input_file):\n",
    "\n",
    "    try:\n",
    "        if os.path.exists('model.h5') and os.path.exists('mapping.pkl'):\n",
    "\n",
    "            with open('mapping.pkl', 'rb') as f:\n",
    "                data_mapping = pickle.load(f)\n",
    "\n",
    "            test_raw = pd.read_csv(input_file)\n",
    "            \n",
    "            test_data, le = process_test_input(\n",
    "                os.path.dirname(input_file), test_raw, data_mapping)\n",
    "\n",
    "            currentDir = os.getcwd()\n",
    "\n",
    "            with open('config.yaml', 'r') as f:\n",
    "                config = yaml.load(f, Loader=yaml.Loader)\n",
    "                models = []\n",
    "                if \"kfold\" in config[\"data\"] and config[\"data\"][\"kfold\"] > 1:\n",
    "                    kfold = config[\"data\"][\"kfold\"]\n",
    "\n",
    "                    if os.path.exists('model.h5'):\n",
    "                        models.append(currentDir + '/model.h5')\n",
    "                    else:\n",
    "                        for sub_run in range(1, kfold + 1):\n",
    "                            sub_dir = currentDir + str(sub_run)\n",
    "                            if os.path.exists(sub_dir + \"/model.h5\"):\n",
    "                                models.append(sub_dir + \"/model.h5\")\n",
    "                else:\n",
    "                    models.append(currentDir + '/model.h5')\n",
    "\n",
    "            result = np.array([])\n",
    "            for modelFile in models:\n",
    "                res = customPredict(test_data, config, modelFile)\n",
    "                if result.size != 0:\n",
    "                    result = res + result\n",
    "                else:\n",
    "                    result = res\n",
    "\n",
    "            res = result / len(models)\n",
    "\n",
    "            out_type = data_mapping['outputs']['OutputPort0']['details'][0]['type']\n",
    "\n",
    "            num_samples = len(test_raw)\n",
    "            if num_samples != 0:\n",
    "                out_dir = \"./\"\n",
    "                if not os.path.exists(out_dir + \"output/\"):\n",
    "                    os.makedirs(out_dir + \"output/\")\n",
    "                if out_type == \"Numpy\":\n",
    "                    if not os.path.exists(out_dir + \"output/\"):\n",
    "                        os.makedirs(out_dir + \"output/\")\n",
    "                    temp = np.ndarray((res.shape[0],), dtype=np.object_)\n",
    "                    for i in range(res.shape[0]):\n",
    "                        filename = \"./output/\" + str(i) + \".npy\"\n",
    "                        np.save(out_dir + filename, res[i])\n",
    "                        temp[i] = filename\n",
    "\n",
    "                    test_raw['predictions'] = temp\n",
    "                elif out_type == 'Array':\n",
    "                    temp = np.ndarray((res.shape[0],), dtype=np.object_)\n",
    "                    res = np.round(res,  decimals=2)\n",
    "                    for i in range(res.shape[0]):\n",
    "                        temp[i] = np.array2string(\n",
    "                            res[i], precision=2, separator=';')\n",
    "                    test_raw['predictions'] = temp\n",
    "                elif out_type == 'Categorical' and le != None:\n",
    "                    res_prob = np.round(\n",
    "                        np.max(res, axis=1).astype(float), decimals=4)\n",
    "                    res_id = np.argmax(res, axis=1)\n",
    "                    res1 = le.inverse_transform(res_id.tolist())\n",
    "                    test_raw['predictions'] = res1\n",
    "                    test_raw['probabilities'] = res_prob\n",
    "                elif out_type == \"Image\":\n",
    "                    temp = np.ndarray((res.shape[0],), dtype=np.object_)\n",
    "                    from tensorflow.keras import backend as K\n",
    "                    if K.image_data_format() == 'channels_first':\n",
    "                        res = np.transpose(res, (0, 2, 3, 1))\n",
    "                        \n",
    "                    for i in range(res.shape[0]):\n",
    "                        filename = \"./output/\" + str(i) + str(round(time.time())) + \".png\"\n",
    "                        if res.shape[-1] == 1:\n",
    "                            img = np.reshape(\n",
    "                                res[i], (res.shape[1], res.shape[2]))\n",
    "                        else:\n",
    "                            img = res[i]\n",
    "                        scipy.misc.imsave(out_dir + filename, img)\n",
    "                        temp[i] = filename\n",
    "\n",
    "                    test_raw['predictions'] = temp\n",
    "                else:\n",
    "                    test_raw['predictions'] = res\n",
    "\n",
    "                test_raw.to_csv('test_result.csv', index=False)\n",
    "\n",
    "        else:\n",
    "            print('model or data mapping does not exist... try downloading again!')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"aborting due to exception... Please check input file format!\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-trinidad",
   "metadata": {},
   "source": [
    "# Run classification by batch (sub-directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running P:/MX ImageStore/Lei-organoid/20210917-HCT116-growth\\20210917-155100-1234_Proj_small\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import timeit\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Column_name = \"Path_TIF\"\n",
    "df = pd.DataFrame()\n",
    "for all_proj in os.listdir(Input_path):\n",
    "    if '_Proj_small' in all_proj:\n",
    "        start_time = timeit.default_timer()\n",
    "        active_dir = os.path.join(Input_path,all_proj)\n",
    "        print(\"Running\",active_dir)\n",
    "        df = pd.DataFrame()\n",
    "        for root, dirs, files in os.walk(active_dir):\n",
    "            for name in files:\n",
    "                if '.tif' in name and 'ch01' in name and '_Proj_small' in root:\n",
    "                    active = pd.Series(os.path.join(root, name),name = Column_name)\n",
    "                    df = df.append(active,ignore_index = True)\n",
    "\n",
    "        df = df.rename(columns={0:Column_name})\n",
    "        DLS_input = os.path.join(active_dir,'test.csv')\n",
    "        df.to_csv(DLS_input, index=False)\n",
    "        test_model(DLS_input)\n",
    "        name = model_name+\".csv\"\n",
    "        shutil.move(os.path.join(model_path,\"test_result.csv\"),os.path.join(active_dir,name))\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(\"Data has been saved at: \",all_proj,\"\\nTime to completion: \",elapsed/60,\"minutes\")\n",
    "        \n",
    "print(\"All tasks completed, have yourself a drink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-rendering",
   "metadata": {},
   "source": [
    "# Run all image files in one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-sally",
   "metadata": {},
   "source": [
    "for really large datasets do no use this option. It will overload the gpu ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "Column_name = \"IJM_method\"\n",
    "df = pd.DataFrame()\n",
    "for root, dirs, files in os.walk(Input_path):\n",
    "    for name in files:\n",
    "        if '.tif' in name and 'ch01' in name and '_Proj' in root:\n",
    "            active = pd.Series(os.path.join(root, name),name = Column_name)\n",
    "            df = df.append(active,ignore_index = True)\n",
    "\n",
    "df = df.rename(columns={0:Column_name})\n",
    "df.to_csv(os.path.join(Input_path,'test.csv'), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLS_input = os.path.join(Input_path,'test.csv')\n",
    "DLS_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import timeit\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = timeit.default_timer()\n",
    "    # imports that depend on backend\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    print(\"Deploying model\")\n",
    "    test_model(DLS_input)\n",
    "    name = model_name+\".csv\"\n",
    "    shutil.move(os.path.join(model_path,\"test_result.csv\"),os.path.join(Input_path,name))\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"Data has been saved at: \",os.path.join(Input_path,name),\"\\nTime to completion: \",elapsed/60,\"minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-burlington",
   "metadata": {},
   "source": [
    "# Release/Clean GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "numerous-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear GPU RAM\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-liquid",
   "metadata": {},
   "source": [
    "# Scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dramatic-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running P:/MX ImageStore/TNBC-Mira/20210730-PIM137-organoid\\20210730-010142-0011_Proj_small\n",
      "Running P:/MX ImageStore/TNBC-Mira/20210730-PIM137-organoid\\20210730-020130-0012_Proj_small\n"
     ]
    }
   ],
   "source": [
    "Column_name = \"IJM_method\"\n",
    "df = pd.DataFrame()\n",
    "for all_proj in os.listdir(Input_path):\n",
    "    if '_Proj_small' in all_proj:\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        active_dir = os.path.join(Input_path,all_proj)\n",
    "        print(\"Running\",active_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = model_name+\".csv\"\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "amber-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caring-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-4a3f57d5652e>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-4a3f57d5652e>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "swiss-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(os.path.join(active_dir,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weird-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P:/MX ImageStore/TNBC-Mira/20210825-PIM172-organoid\\\\20210827-000237-034417_Proj_small\\\\RESNET18.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(active_dir,name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

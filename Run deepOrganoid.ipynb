{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403fbc3f",
   "metadata": {},
   "source": [
    "# deepOrganoid: Brightfeld viability assay for matrix embedded organoids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb9192",
   "metadata": {},
   "source": [
    "Select the first box and press SHIFT+ENTER. This will run that block and advance to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "irish-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries have been loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import scipy.io.wavfile\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from multiprocessing import cpu_count\n",
    "from matplotlib.pyplot import imread\n",
    "from PIL import Image\n",
    "print(\"All libraries have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65236812",
   "metadata": {},
   "source": [
    "**The subsequent box should match the following:**<br />\n",
    "Tensorflow: 2.1.0<br />\n",
    "nvcc: NVIDIA (R) Cuda compiler driver<br />\n",
    "Copyright (c) 2005-2020 NVIDIA Corporation<br />\n",
    "Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020<br />\n",
    "Cuda compilation tools, release 11.0, V11.0.194<br />\n",
    "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "biological-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.1.0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.194\n",
      "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow: \",tf.__version__)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-needle",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bound-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must download models first\n",
    "#Options: AutoML, Lite_vr1, Lite_vr2, Lite_vr3, Final. Refer to text for differences\n",
    "rel_model_path = 'Models/AutoML'\n",
    "\n",
    "#Base directory with the images\n",
    "#WILL NEED TO BE EDITED, For windows remember to use either \\\\ or / as path deliminters \n",
    "rel_Input_path = 'Example_data' \n",
    "\n",
    "#The enumerated GPU device you wish to use 0,1,nGPU\n",
    "cuda_device = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-above",
   "metadata": {},
   "source": [
    "# deepOrganoid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beautiful-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  Q:/CTCR_DeepHTS/deepOrganoid/Models/AutoML\n",
      "Found:  Q:/CTCR_DeepHTS/deepOrganoid/Example_data\n"
     ]
    }
   ],
   "source": [
    "Base = os.getcwd()\n",
    "Base = 'Q:/CTCR_DeepHTS/deepOrganoid/'\n",
    "model_path = os.path.join(Base,rel_model_path)\n",
    "Input_path = os.path.join(Base,rel_Input_path)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Found: \",model_path)\n",
    "    os.chdir(model_path)\n",
    "    model_name = os.path.basename(model_path)\n",
    "else:\n",
    "    print('Model path not found')\n",
    "    \n",
    "if os.path.exists(Input_path):\n",
    "    print(\"Found: \",Input_path)\n",
    "else:\n",
    "    print('Image directory not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sudden-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"MKL_NUM_THREADS\"] = str(cpu_count())\n",
    "os.environ[\"MKL_NUM_THREADS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e5208",
   "metadata": {},
   "source": [
    "# Import inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "important-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doResize(options):\n",
    "    resize = None\n",
    "    if options and 'Resize' in options and options['Resize'] == True:\n",
    "        resize = (int(options['Width']), int(options['Height']))\n",
    "    return resize\n",
    "\n",
    "def col_pre_process(data, options):\n",
    "    if len(options.keys()) == 0:\n",
    "        return data\n",
    "    else:\n",
    "        if \"Scaling\" in options and float(options[\"Scaling\"]) != 0 and float(options[\"Scaling\"]) != 1:\n",
    "            data = data / float(options[\"Scaling\"])\n",
    "\n",
    "        if 'Normalization' in options and options['Normalization'] == True:\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            data = data - mean\n",
    "            data = data / std\n",
    "            return data\n",
    "        return data\n",
    "\n",
    "def process_test_input(base_dir, test_raw, data_mapping):\n",
    "    test_data = []\n",
    "    le = None\n",
    "\n",
    "    from tensorflow.keras import backend as K\n",
    "    if K.backend() == 'theano' or K.backend() == 'mxnet':\n",
    "        K.set_image_data_format('channels_first')\n",
    "    else:\n",
    "        K.set_image_data_format('channels_last')\n",
    "\n",
    "    # determine the shape of the data to feed into the network\n",
    "    for i in range(len(data_mapping['inputs'])):\n",
    "        inp_port = data_mapping['inputs']['InputPort' + str(i)]\n",
    "        if inp_port['details'][0]['type'] == 'Image':\n",
    "            col_name = inp_port['details'][0]['name']\n",
    "            if 'options' in inp_port['details'][0]:\n",
    "                options = inp_port['details'][0]['options']\n",
    "            else:\n",
    "                options = {}\n",
    "\n",
    "            resize = doResize(options)\n",
    "            img = imread(test_raw[col_name][0])\n",
    "            input_shape = img.shape\n",
    "\n",
    "            num_channels = 1\n",
    "            if resize:\n",
    "                width, height = resize\n",
    "                if len(input_shape) == 3:\n",
    "                    num_channels = 3\n",
    "            else:\n",
    "                if len(input_shape) == 2:\n",
    "                    width, height = input_shape\n",
    "                else:\n",
    "                    width, height, num_channels = input_shape\n",
    "            test_data.append(np.ndarray((len(test_raw),) +\n",
    "                                        (num_channels, width, height), dtype=np.float32))\n",
    "\n",
    "            for j, filename in enumerate(test_raw[col_name]):\n",
    "                img = imread(filename)\n",
    "                if resize:\n",
    "                    img = np.array(Image.fromarray(img.astype(np.uint8)).resize(resize))\n",
    "                if num_channels != 1:\n",
    "                    img = np.transpose(img, (2, 0, 1))\n",
    "                test_data[i][j] = img\n",
    "\n",
    "            if K.image_data_format() == 'channels_last':\n",
    "                test_data[i] = np.transpose(test_data[i], (0, 2, 3, 1))\n",
    "\n",
    "            test_data[i] = col_pre_process(test_data[i], options)\n",
    "\n",
    "    # assuming single output, generate labelEncoder\n",
    "    out_port = data_mapping['outputs']['OutputPort0']\n",
    "    if out_port['details'][0]['type'] == 'Categorical':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(out_port['details'][0]['categories'])\n",
    "    return test_data, le\n",
    "\n",
    "def customPredict(test_data, config, modelFile):\n",
    "    res = None\n",
    "    loss_func = config['params']['loss_func']\n",
    "    if 'is_custom_loss' in config['params']:\n",
    "        isCustomLoss = config['params']['is_custom_loss']\n",
    "    else:\n",
    "        isCustomLoss = False\n",
    "    if isCustomLoss:\n",
    "        customLoss = SourceFileLoader(\n",
    "            \"customLoss\", 'customLoss.py').load_module()\n",
    "        loss_function = eval('customLoss.' + loss_func)\n",
    "        mod = load_model(modelFile, custom_objects={loss_func: loss_function})\n",
    "    else:\n",
    "        mod = load_model(modelFile)\n",
    "    \n",
    "    if os.environ.get(\"GPU_ENABLED\", \"0\") == \"1\":\n",
    "        mod.compile (loss='categorical_crossentropy', optimizer='adam', context=[\"GPU(\"+str(cuda_device)+\")\"])\n",
    "    with tf.device(\"GPU:\"+str(cuda_device)):\n",
    "        ress = mod.predict(test_data,batch_size=4)\n",
    "    return ress\n",
    "\n",
    "def test_model(input_file):\n",
    "    try:\n",
    "        if os.path.exists('model.h5') and os.path.exists('mapping.pkl'):\n",
    "            with open('mapping.pkl', 'rb') as f:\n",
    "                data_mapping = pickle.load(f)\n",
    "            test_raw = pd.read_csv(input_file)\n",
    "            test_data, le = process_test_input(\n",
    "                os.path.dirname(input_file), test_raw, data_mapping)\n",
    "            currentDir = os.getcwd()\n",
    "            with open('config.yaml', 'r') as f:\n",
    "                config = yaml.load(f, Loader=yaml.Loader)\n",
    "                models = []\n",
    "                models.append(currentDir + '/model.h5')\n",
    "\n",
    "            result = np.array([])\n",
    "            for modelFile in models:\n",
    "                res = customPredict(test_data, config, modelFile)\n",
    "                if result.size != 0:\n",
    "                    result = res + result\n",
    "                else:\n",
    "                    result = res\n",
    "            res = result / len(models)\n",
    "\n",
    "            out_type = data_mapping['outputs']['OutputPort0']['details'][0]['type']\n",
    "\n",
    "            num_samples = len(test_raw)\n",
    "            if num_samples != 0:\n",
    "                out_dir = \"./\"\n",
    "                if not os.path.exists(out_dir + \"output/\"):\n",
    "                    os.makedirs(out_dir + \"output/\")\n",
    "                if out_type == \"Numpy\":\n",
    "                    if not os.path.exists(out_dir + \"output/\"):\n",
    "                        os.makedirs(out_dir + \"output/\")\n",
    "                    temp = np.ndarray((res.shape[0],), dtype=np.object_)\n",
    "                    for i in range(res.shape[0]):\n",
    "                        filename = \"./output/\" + str(i) + \".npy\"\n",
    "                        np.save(out_dir + filename, res[i])\n",
    "                        temp[i] = filename\n",
    "                    test_raw['predictions'] = temp\n",
    "                test_raw.to_csv('test_result.csv', index=False)\n",
    "        else:\n",
    "            print('model or data mapping does not exist... try downloading again!')\n",
    "    except Exception as e:\n",
    "        print(\"aborting due to exception... Please check input file format!\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-trinidad",
   "metadata": {},
   "source": [
    "# Run classification by batch (sub-directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tested-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building input:  Q:/CTCR_DeepHTS/deepOrganoid/Example_data\\2019-04-29_Proj\n",
      "Running deepOrganoid\n",
      "Data has been saved at:  2019-04-29_Proj \n",
      "Time to completion:  0.786632441666734 minutes\n",
      "All tasks completed, have yourself a drink\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import timeit\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if os.path.exists('model.h5') and os.path.exists('mapping.pkl'):\n",
    "    with open('mapping.pkl', 'rb') as f:\n",
    "        data_mapping = pickle.load(f)\n",
    "    \n",
    "    for i in range(len(data_mapping['inputs'])):\n",
    "        inp_port = data_mapping['inputs']['InputPort' + str(i)]\n",
    "        if inp_port['details'][0]['type'] == 'Image':\n",
    "            Column_name = inp_port['details'][0]['name']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for all_proj in os.listdir(Input_path):\n",
    "    if '_Proj' in all_proj: ##These conditional statments may need to be edited for new applications\n",
    "        start_time = timeit.default_timer()\n",
    "        active_dir = os.path.join(Input_path,all_proj)\n",
    "        \n",
    "        name = model_name+\".csv\"\n",
    "        skip = os.path.isfile(os.path.join(active_dir,name))\n",
    "        \n",
    "        if skip:\n",
    "            print(\"Skipping\",active_dir)\n",
    "        else:\n",
    "            print(\"Building input: \",active_dir)\n",
    "            df = pd.DataFrame()\n",
    "            for root, dirs, files in os.walk(active_dir):\n",
    "                for name in files:\n",
    "                    if '.tif' in name and 'ch01' in name:\n",
    "                        active = pd.Series(os.path.join(root, name),name = Column_name)\n",
    "                        df = df.append(active,ignore_index = True)\n",
    "            \n",
    "            df = df.rename(columns={0:Column_name})\n",
    "            DLS_input = os.path.join(active_dir,'test.csv')\n",
    "            df.to_csv(DLS_input, index=False)\n",
    "            \n",
    "            print(\"Running deepOrganoid\")\n",
    "            test_model(DLS_input)\n",
    "            name = model_name+\".csv\"\n",
    "            shutil.move(os.path.join(model_path,\"test_result.csv\"),os.path.join(active_dir,name))\n",
    "            os.remove(DLS_input)\n",
    "            \n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            print(\"Data has been saved at: \",all_proj,\"\\nTime to completion: \",elapsed/60,\"minutes\")\n",
    "\n",
    "print(\"All tasks completed, have yourself a drink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-burlington",
   "metadata": {},
   "source": [
    "# Release/Clean GPU VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerous-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear GPU RAM\n",
    "from numba import cuda\n",
    "for index, device in enumerate(cuda.gpus):\n",
    "    cuda.select_device(index)\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-liquid",
   "metadata": {},
   "source": [
    "# Scratch space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
